{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Markov Chain Model\n",
    "\n",
    "The Markov chain (MC) model is the second model used in our thesis. It is significantly more complex than the simple probabilistic model, however, it is still quite rudimentary in comparison with real-life markets. The model was developed by Hult and Kiessling in their paper *[Algorithmic trading with Markov Chains](https://www.researchgate.net/publication/268032734_ALGORITHMIC_TRADING_WITH_MARKOV_CHAINS)*.\n",
    "\n",
    "In this model, the limit order book (LOB) is modelled explicitly. There are six event types:\n",
    "\n",
    "> 1. Buy limit orders \n",
    "> 2. Sell limit orders\n",
    "> 3. Cancel buy orders\n",
    "> 4. Cancel sell orders\n",
    "> 5. Buy market orders\n",
    "> 6. Sell market orders\n",
    "\n",
    "The arrival of an order results in a state transition in the Markov chain. The transition rates are described in our **[report](https://github.com/KodAgge/Reinforcement-Learning-for-Market-Making/blob/main/Reinforcement%20Learning%20for%20Market%20Making.pdf)**. An example of how the arrival of different orders affect the LOB is shown in the image below.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/LOBDynamics.png\" width=800/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Like in the simple probabilistic model we also have:\n",
    "\n",
    "> * The time _t_ can take integer values between _0_ and _T_.\n",
    ">\n",
    "> * The market maker has to quote bid and ask prices every second.\n",
    ">\n",
    "> * The market maker can put the bid and ask depths at *max\\_quote\\_depth* different levels, from _1_ to *max\\_quote\\_depth* ticks away from the best ask and best bid price respectively.\n",
    ">\n",
    "> * The cash process _X<sub>t</sub>_ denotes the market makers cash at time _t_.\n",
    ">\n",
    "> * The inventory process _Q<sub>t</sub>_ denotes the market makers inventory at time _t_.\n",
    ">\n",
    "> * The value process _V<sub>t</sub>_ denotes the value of the market maker's position at time _t_, that is its cash plus the value of its current inventory.\n",
    ">\n",
    "> * The market maker can see the current time _t_ , its inventory _Q<sub>t</sub>_, the spread and the *full LOB* before taking an action.\n",
    ">\n",
    "> * At time _t = T_ the market maker is forced to liquidate its position.\n",
    "\n",
    "The _tick_ is the smallest tradeable unit of the underlying, for instance $0.01 of AAPL.\n",
    "\n",
    "Contrary to the simple probabilistic model, it is not possible to derive an analytically optimal strategy in the Markov chain model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The deep reinforcement learning\n",
    "\n",
    "After that short introduction, it's time for some deep reinforcement learning in the form of DDQN.\n",
    "\n",
    "We start by importing the needed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from mc_model_mm_deep_rl_batch import (\n",
    "    train_multiple_agents_batch,\n",
    "    evaluate_DDQN_batch\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to decide on the parameters we want to use for the environment and the hyperparameters we want to use for the DDQN.\n",
    "\n",
    "There are some additional parameters in the Markov chain model, see the code snippet below for an explanation of them. However, importantly, we choose a longer episode (trading window) of *T = 100*.\n",
    "\n",
    "Since DDQN involves neural networks, even more parameters are added. A scaling of the reward is needed to try keep it within *\\[-1,1\\]*. Also, since the market maker can view the full LOB, we let it reset to a random state at the start of every episode to let the agent get exposed to more LOB states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "                \"dt\": 1,                    # the length of the time steps\n",
    "                \"T\": 100,                   # the length of the episode\n",
    "                \"num_levels\": 10,           # how many depth levels that should be included in the LOB\n",
    "                \"default_order_size\": 5,    # the size of the orders the MM places\n",
    "                \"max_quote_depth\": 5,       # how deep the MM can put its quotes\n",
    "                \"reward_scale\": 0.1,        # a factor all rewards will be multiplied with\n",
    "                \"randomize_reset\": True     # should a random LOB state  be chosen at the start of every episode?\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to decide which hyperparameter values we want to use.\n",
    "\n",
    "The DDQN algorithm involves a lot of hyperparameters. We will not explain them here, instead we direct the interest reader to our report **[report](https://github.com/KodAgge/Reinforcement-Learning-for-Market-Making/blob/main/Reinforcement%20Learning%20for%20Market%20Making.pdf)**. However, they do involve network architecture, experience replay and epsilon-greedy policy parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "                \"n_train\": int(2e5),    # the number of steps the agents will be trained for\n",
    "                \"n_test\": int(1e2),     # the number of episodes the agents will be evaluated for\n",
    "                \"n_runs\": 4             # the number of agents that will be trained\n",
    "}\n",
    "\n",
    "DDQN_params = {\n",
    "                # network params\n",
    "                \"hidden_size\": 64,                                          # the hidden size of the network\n",
    "                \"buffer_size\": hyperparams[\"n_train\"] / 200,                # the size of the experience replay bank\n",
    "                \"replay_start_size\": hyperparams[\"n_train\"] / 200,          # after how many number of steps the experience replay is started\n",
    "                \"target_update_interval\": hyperparams[\"n_train\"] / 100,     # how often the target network is updated\n",
    "                \"update_interval\": 2,                                       # how often the online network is updated\n",
    "                \"minibatch_size\": 16,                                       # the size of the minibatches used\n",
    "\n",
    "                # epsilon greedy (linear decay)\n",
    "                \"exploration_initial_eps\": 1,                               # the starting value of the exploration rate\n",
    "                \"exploration_final_eps\": 0.05,                              # the final value of the exploration rate\n",
    "                \"exploration_fraction\": 0.5,                                # when the final value is reached\n",
    "\n",
    "                # learning rate\n",
    "                \"learning_rate_dqn\": 1e-4,                                  # the learning rate used (Adam)\n",
    "                \n",
    "                # other params\n",
    "                \"num_envs\": 10,                                             # how many parallelized environments\n",
    "                \"n_train\": hyperparams[\"n_train\"], \n",
    "                \"n_runs\": hyperparams[\"n_runs\"],\n",
    "                \"reward_scale\": model_params[\"reward_scale\"],\n",
    "\n",
    "                # logging params\n",
    "                \"log_interval\": hyperparams[\"n_train\"] / 100,               # the frequency of saving information\n",
    "                \"num_estimate\": 10000,                                      # how many states that should be used for estimating q_values\n",
    "                \"n_states\": 10                                              # the number of states heatmaps are averaged over\n",
    "                \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model it is the emulating of the market that is the bottleneck, so it runs faster on a cpu than a gpu. This holds even when multithreading is used for the emulation, which we use in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we decide where to save our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming the folder where the results will be saved\n",
    "folder_name = \"mc_deep_example\"\n",
    "\n",
    "outdir = f\"results/mc_model_deep/{folder_name}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready for the deep reinforcement learning!\n",
    "\n",
    "This is easily done with the function *train\\_multiple\\_agents\\_batch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mc_model_mm_deep_rl_batch:The folder results/mc_model_deep/mc_deep_example/estimate_folder already exists.\n",
      "INFO:mc_model_mm_deep_rl_batch:Run 1 in progress.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get local object 'setup_batch_env.<locals>.make_env'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_multiple_agents_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDDQN_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_runs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpu\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maver\\Reinforcement-Learning-for-Market-Making\\code\\mc_model_mm_deep_rl_batch.py:378\u001b[0m, in \u001b[0;36mtrain_multiple_agents_batch\u001b[1;34m(info, args, n, outdir, n_runs, gpu)\u001b[0m\n\u001b[0;32m    376\u001b[0m env_function \u001b[38;5;241m=\u001b[39m get_env_function()\n\u001b[0;32m    377\u001b[0m sample_env \u001b[38;5;241m=\u001b[39m env_function(args)\n\u001b[1;32m--> 378\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m \u001b[43msetup_batch_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# Setup the agent\u001b[39;00m\n\u001b[0;32m    381\u001b[0m agent \u001b[38;5;241m=\u001b[39m setup_ddqn_agent(vec_env, info, gpu)\n",
      "File \u001b[1;32mc:\\Users\\maver\\Reinforcement-Learning-for-Market-Making\\code\\mc_model_mm_deep_rl_batch.py:2126\u001b[0m, in \u001b[0;36msetup_batch_env\u001b[1;34m(args, env_function, num_envs, seed)\u001b[0m\n\u001b[0;32m   2123\u001b[0m     env\u001b[38;5;241m.\u001b[39mseed(env_seed)\n\u001b[0;32m   2124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env\n\u001b[1;32m-> 2126\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m \u001b[43mpfrl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiprocessVectorEnv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunctools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmake_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vec_env\n",
      "File \u001b[1;32mc:\\Users\\maver\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pfrl\\envs\\multiprocess_vector_env.py:64\u001b[0m, in \u001b[0;36mMultiprocessVectorEnv.__init__\u001b[1;34m(self, env_fns)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mps \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     60\u001b[0m     Process(target\u001b[38;5;241m=\u001b[39mworker, args\u001b[38;5;241m=\u001b[39m(work_remote, env_fn))\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (work_remote, env_fn) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwork_remotes, env_fns)\n\u001b[0;32m     62\u001b[0m ]\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mps:\n\u001b[1;32m---> 64\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_obs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_spaces\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\maver\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maver\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maver\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\context.py:337\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maver\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\maver\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get local object 'setup_batch_env.<locals>.make_env'"
     ]
    }
   ],
   "source": [
    "train_multiple_agents_batch(\n",
    "    DDQN_params, \n",
    "    model_params, \n",
    "    hyperparams[\"n_train\"], \n",
    "    outdir, \n",
    "    hyperparams[\"n_runs\"], \n",
    "    gpu=gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the strategies\n",
    "\n",
    "Now that the training is complete, we can now continue with evaluating the agents.\n",
    "\n",
    "This is easily done with the function *evaluate\\_DDQN\\_batch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mc_model_mm_deep_rl_batch:The folder results/mc_model_deep/mc_deep_example/image_folder already exists.\n",
      "INFO:mc_model_mm_deep_rl_batch:Plotting training.\n",
      "INFO:mc_model_mm_deep_rl_batch:Plotting strategies.\n",
      "INFO:mc_model_mm_deep_rl_batch:Evaluating agents.\n",
      "INFO:mc_model_mm_deep_rl_batch:Evaluating benchmarks...\n",
      "INFO:mc_model_mm_deep_rl_batch:...best agent\n",
      "INFO:mc_model_mm_deep_rl_batch:...mean agent\n",
      "INFO:mc_model_mm_deep_rl_batch:...constant strategy\n",
      "INFO:mc_model_mm_deep_rl_batch:...random_strategy\n",
      "INFO:mc_model_mm_deep_rl_batch:Visualizing the strategies.\n",
      "INFO:mc_model_mm_deep_rl_batch:The folder results/mc_model_deep/mc_deep_example/image_folder already exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_DDQN_batch(\n",
    "    outdir, \n",
    "    n_test=hyperparams[\"n_test\"],                  \n",
    "    Q=10,       # how many depths that should be displayed in the heatmaps\n",
    "    randomize_start=model_params[\"randomize_reset\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Evaluation complete! Let's take a peek at the images that were saved when running *evaluate\\_DDQN\\_batch*.\n",
    "\n",
    "Let's first have a look at the reward, the estimated state-value at (0,0) and the network loss during training.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model_deep/mc_deep_example/image_folder/training_graph.png\"/>\n",
    "</div>\n",
    "\n",
    "In this image it looks like that the algorithm hasn't converged. Indeed, it has to be trained for much longer. It probably also needs hyperparameter tuning since the q-estimate and the loss seems to be diverging.\n",
    "\n",
    "We can also have a look the learnt strategies. The figure below shows the learnt bid depths.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model_deep/mc_deep_example/image_folder/bid_heat_randomized_10.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Furthermore, we can compare the average rewards of the Q-learning strategies versus some benchmarking strategies. These are displayed in the boxplot below.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model_deep/mc_deep_example/image_folder/box_plot_benchmarking.png\"/>\n",
    "</div>\n",
    "\n",
    "We can also view these results in table form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy           mean reward    std reward    reward per action    reward per second\n",
      "---------------  -------------  ------------  -------------------  -------------------\n",
      "constant (d=1)          0.0402     0.104441              0.000402             0.000402\n",
      "random                 -0.013      0.075743             -0.00013             -0.00013\n",
      "DDQN (best run)         0.0254     0.0751188             0.000254             0.000254\n",
      "DDQN (mean)             0.014      0.0967264             0.00014              0.00014\n"
     ]
    }
   ],
   "source": [
    "f = open(f\"{outdir}image_folder/table_benchmarking\")\n",
    "print(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could a be interesting to see how the mean strategy and the individual strategies act. The figures below shows the average inventory, cash and value process of the different strategies when evaluted for *n\\_test* episodes.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model_deep/mc_deep_example/image_folder/visualization_mean.png\"/>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model_deep/mc_deep_example/image_folder/visualization_all.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More results?\n",
    "\n",
    "There are a lot more figures and tables to explore which can be found in the **[mc_deep_example](https://github.com/KodAgge/Reinforcement-Learning-for-Market-Making/tree/main/code/results/mc_model_deep/mc_deep_example/image_folder)** folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
