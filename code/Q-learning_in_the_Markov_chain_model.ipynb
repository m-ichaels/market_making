{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Markov Chain Model\n",
    "\n",
    "The Markov chain (MC) model is the second model used in our thesis. It is significantly more complex than the simple probabilistic model, however, it is still quite rudimentary in comparison with real-life markets. The model was developed by Hult and Kiessling in their paper *[Algorithmic trading with Markov Chains](https://www.researchgate.net/publication/268032734_ALGORITHMIC_TRADING_WITH_MARKOV_CHAINS)*.\n",
    "\n",
    "In this model, the limit order book (LOB) is modelled explicitly. There are six event types:\n",
    "\n",
    "> 1. Buy limit orders \n",
    "> 2. Sell limit orders\n",
    "> 3. Cancel buy orders\n",
    "> 4. Cancel sell orders\n",
    "> 5. Buy market orders\n",
    "> 6. Sell market orders\n",
    "\n",
    "The arrival of an order results in a state transition in the Markov chain. The transition rates are described in our **[report](https://github.com/KodAgge/Reinforcement-Learning-for-Market-Making/blob/main/Reinforcement%20Learning%20for%20Market%20Making.pdf)**. An example of how the arrival of different orders affect the LOB is shown in the image below.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/LOBDynamics.png\" width=800/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Like in the simple probabilistic model we also have:\n",
    "\n",
    "> * The time _t_ can take integer values between _0_ and _T_.\n",
    ">\n",
    "> * The market maker has to quote bid and ask prices every second.\n",
    ">\n",
    "> * The market maker can put the bid and ask depths at *max\\_quote\\_depth* different levels, from _1_ to *max\\_quote\\_depth* ticks away from the best ask and best bid price respectively.\n",
    ">\n",
    "> * The cash process _X<sub>t</sub>_ denotes the market makers cash at time _t_.\n",
    ">\n",
    "> * The inventory process _Q<sub>t</sub>_ denotes the market makers inventory at time _t_.\n",
    ">\n",
    "> * The value process _V<sub>t</sub>_ denotes the value of the market maker's position at time _t_, that is its cash plus the value of its current inventory.\n",
    ">\n",
    "> * The market maker can see the current time _t_ and its inventory before taking an action. However, the inventory (_Q<sub>t</sub>_) is grouped into bins, whose size is decided by the parameter $\\kappa$.\n",
    ">\n",
    "> * At time _t = T_ the market maker is forced to liquidate its position.\n",
    "\n",
    "The _tick_ is the smallest tradeable unit of the underlying, for instance $0.01 of AAPL.\n",
    "\n",
    "Contrary to the simple probabilistic model, it is not possible to derive an analytically optimal strategy in the Markov chain model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Q-learning\n",
    "\n",
    "After that short introduction, it's time for some reinforcement learning in the form of Q-learning.\n",
    "\n",
    "We start by importing the needed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import the Q-learning file for the markov chain model\n",
    "from mc_model_evaluation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to decide on the parameters we want to use for the environment and the hyperparameters we want to use for the Q-learning.\n",
    "\n",
    "There are some additional parameters in the Markov chain model, see the code snippet below for an explanation of them. However, importantly, we choose a longer episode (trading window) of *T = 100*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "                \"dt\": 1,                    # the length of the time steps\n",
    "                \"T\": 100,                   # the length of the episode\n",
    "                \"num_time_buckets\": 100,    # how many bins that should be used for the time\n",
    "                \"kappa\": 3,                 # the size of the inventory bins\n",
    "                \"num_levels\": 10,           # how many depth levels that should be included in the LOB\n",
    "                \"default_order_size\": 5,    # the size of the orders the MM places\n",
    "                \"max_quote_depth\": 5,       # how deep the MM can put its quotes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to decide which hyperparameter values we want to use. There is not much to choose here. We have to decide on the parameter schemes for the epislon-greedy policy and the learning rate. Finally we need to decide how long we want to train for, how many times we want to train and how long we want to evalute for.\n",
    "\n",
    "> *\\_start* indicates the starting value of the parameter.\n",
    ">\n",
    "> *\\_end* indicates the final value of the parameter.\n",
    ">\n",
    "> *\\_cutoff* indicates when the final value is reached, i.e. 0.5 means after 50% of the training.\n",
    "\n",
    "**Note:** We don't use exploring starts in this setting. See our **[report](https://github.com/KodAgge/Reinforcement-Learning-for-Market-Making/blob/main/Reinforcement%20Learning%20for%20Market%20Making.pdf)** for the reason why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_learning_params = {\n",
    "        # epsilon-greedy values (linear decay)\n",
    "        \"epsilon_start\": 1,\n",
    "        \"epsilon_end\": 0.05,\n",
    "        \"epsilon_cutoff\": 0.5,\n",
    "\n",
    "        # learning-rate values (exponential decay)\n",
    "        \"alpha_start\": 0.5,\n",
    "        \"alpha_end\": 0.001,\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "        \"n_train\" : 3e3,\n",
    "        \"n_test\" : 3e2,\n",
    "        \"n_runs\" : 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we decide where to save our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming the folder where the results will be saved\n",
    "folder_mode = True\n",
    "folder_name = \"mc_example\"\n",
    "save_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready for the Q-learning!\n",
    "\n",
    "This is easily done with the function *Q\\_learning\\_comparison*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN 1 IN PROGRESS...\n",
      "\tEpisode 600 (20%), 0:04:02.470000 remaining of this run\n",
      "\tEpisode 1200 (40%), 0:03:00.580000 remaining of this run\n",
      "\tEpisode 1800 (60%), 0:01:59.500000 remaining of this run\n",
      "\tEpisode 2400 (80%), 0:00:59.630000 remaining of this run\n",
      "\tEpisode 3000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER mc_example ALREADY EXISTS\n",
      "...FINISHED IN 0:04:58.260000\n",
      "0:14:54.790000 REMAINING OF THE TRAINING\n",
      "========================================\n",
      "RUN 2 IN PROGRESS...\n",
      "\tEpisode 600 (20%), 0:03:58.930000 remaining of this run\n",
      "\tEpisode 1200 (40%), 0:02:58.210000 remaining of this run\n",
      "\tEpisode 1800 (60%), 0:01:56.890000 remaining of this run\n",
      "\tEpisode 2400 (80%), 0:00:58.010000 remaining of this run\n",
      "\tEpisode 3000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER mc_example ALREADY EXISTS\n",
      "...FINISHED IN 0:04:47.830000\n",
      "0:09:35.650000 REMAINING OF THE TRAINING\n",
      "========================================\n",
      "RUN 3 IN PROGRESS...\n",
      "\tEpisode 600 (20%), 0:03:52.790000 remaining of this run\n",
      "\tEpisode 1200 (40%), 0:02:56.420000 remaining of this run\n",
      "\tEpisode 1800 (60%), 0:01:59.340000 remaining of this run\n",
      "\tEpisode 2400 (80%), 0:00:59.180000 remaining of this run\n",
      "\tEpisode 3000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER mc_example ALREADY EXISTS\n",
      "...FINISHED IN 0:04:56.080000\n",
      "0:04:56.080000 REMAINING OF THE TRAINING\n",
      "========================================\n",
      "RUN 4 IN PROGRESS...\n",
      "\tEpisode 600 (20%), 0:04:07.460000 remaining of this run\n",
      "\tEpisode 1200 (40%), 0:03:01.970000 remaining of this run\n",
      "\tEpisode 1800 (60%), 0:01:58.840000 remaining of this run\n",
      "\tEpisode 2400 (80%), 0:00:58.820000 remaining of this run\n",
      "\tEpisode 3000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER mc_example ALREADY EXISTS\n",
      "...FINISHED IN 0:04:51.770000\n",
      "========================================\n",
      "FULL TRAINING COMPLETED IN 0:19:33.930000\n",
      "\n",
      "PLOTTING REWARDS... DONE\n",
      "\n",
      "EVALUATING DIFFERENT Q-STRATEGIES.... DONE\n",
      "\n",
      "EVALUATING DIFFERENT STRATEGIES.... DONE\n",
      "\n",
      "SHOWING STRATEGIES... DONE\n",
      "\n",
      "SHOWING STD FOR Q MATRIX\n",
      "\n",
      "HEATMAP FOR ERRORS\n"
     ]
    }
   ],
   "source": [
    "Q_learning_comparison(\n",
    "    **hyperparams,\n",
    "    args=model_params,\n",
    "    Q_learning_args=Q_learning_params,\n",
    "    folder_mode = folder_mode,\n",
    "    folder_name = folder_name,\n",
    "    save_mode = save_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the strategies\n",
    "\n",
    "We can now have a look at the images that were saved when running *Q\\_learning\\_comparison*.\n",
    "\n",
    "Let's first have a look at the reward and the state-value at (0,0) during training.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model/mc_example/results_graph.png\"/>\n",
    "</div>\n",
    "\n",
    "In this image it looks like that the Q-learning has converged, however, it has not. It has to be trained for *much* longer. This will become very evident from the images below.\n",
    "\n",
    "We can also have a look the learnt strategies. The figure below shows the learnt bid depths.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model/mc_example/opt_bid_heat.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Furthermore, we can compare the average rewards of the Q-learning strategies versus some benchmarking strategies. These are displayed in the boxplot below.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model/mc_example/box_plot_benchmarking.png\"/>\n",
    "</div>\n",
    "\n",
    "We can also view these results in table form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy                 mean reward    std reward\n",
      "---------------------  -------------  ------------\n",
      "constant (d=1)              1.77           8.81346\n",
      "random                     -0.423333       7.72123\n",
      "Q_learning (best run)      -0.48           5.63941\n",
      "Q_learning (average)       -0.79           5.94019\n"
     ]
    }
   ],
   "source": [
    "f = open(\"results/mc_model/mc_example/table_benchmarking\")\n",
    "print(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results it's clear that we have to train for much longer before we can find any good strategies. Interestingly the constant strategy severly outperforms all other strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More results?\n",
    "\n",
    "There are a lot more figures and tables to explore which can be found in the **[mc_example](https://github.com/KodAgge/Reinforcement-Learning-for-Market-Making/tree/main/code/results/mc_model/mc_example)** folder."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83f73d5875a575e504ba23451a5997fea59c0c75034f677431fe9f5bc2b0207e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
