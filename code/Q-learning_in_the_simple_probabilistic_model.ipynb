{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Simple Probabilistic Model\n",
    "\n",
    "The simple probabilistic model (SPM) is the first model used in our thesis. It is a discretization of the model presentented in Chapter 10.2 in Cartea et al.'s book _Algorithmic and High-Frequency Trading_.\n",
    "\n",
    "It can be summarized as follows (for the full definition, see our **[report](https://github.com/KodAgge/Reinforcement-Learning-for-Market-Making/blob/main/Reinforcement%20Learning%20for%20Market%20Making.pdf)**):\n",
    "> * The time _t_ can take integer values between _0_ and _T_.\n",
    ">\n",
    "> * The midprice _S<sub>t</sub>_ is a Brownian motion rounded to the closest tick.\n",
    ">\n",
    "> * The market maker has to quote bid and ask prices every second.\n",
    ">\n",
    "> * The market maker can put the bid and ask depths at _d_ different levels, from _0_ to _d - 1_ ticks away from the mid price.\n",
    ">\n",
    "> * The cash process _X<sub>t</sub>_ denotes the market maker's cash at time _t_.\n",
    ">\n",
    "> * The inventory process _Q<sub>t</sub>_ denotes the market maker's inventory at time _t_.\n",
    ">\n",
    "> * The value process _V<sub>t</sub>_ denotes the value of the market maker's position at time _t_, that is its cash plus the value of its current inventory.\n",
    ">\n",
    "> * The market maker can see the current time and its inventory _(t,Q<sub>t</sub>)_ before taking an action.\n",
    ">\n",
    "> * At time _t = T_ the market maker is forced to liquidate its position.\n",
    "\n",
    "The _tick_ is the smallest tradeable unit of the underlying, for instance $0.01 of AAPL.\n",
    "\n",
    "Based on Cartea et al.'s definition, an analytically optimal strategy can be defined, with which we want to compare strategies derived with Q-learning. However, there is no guarantee that these strategies are optimal in the discretized version of the model.\n",
    "\n",
    "An example of the optimal bid depths for a specific set of model parameters is shown in the figure below. **Note** that these depths are _not_ discretized in terms of depth, that is they're not rounded to the closest tick.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/ContinuousBid30.png\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Q-learning\n",
    "\n",
    "After that short introduction, it's time for some reinforcement learning in the form of Q-learning.\n",
    "\n",
    "We start by importing the needed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import the Q-learning file for the simple probabilistic model\n",
    "from simple_model_evaluation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to decide on the parameters we want to use for the environment and the hyperparameters we want to use for the Q-learning.\n",
    "\n",
    "For the model we choose an episode length of *T = 20* and a running inventory penalty of *$\\phi$ = 10<sup>-4</sup>*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "                \"d\": 4,         # the number of different quotation depths that can be chosen from\n",
    "                \"T\": 20,        # the length of the episode\n",
    "                \"dp\": 0.01,     # the tick size\n",
    "                \"min_dp\": 0,    # the minimum number of ticks from the mid price that is allowed to put prices at\n",
    "                \"phi\": 1e-4     # the running inventory penalty\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to decide which hyperparameter values we want to use.\n",
    "\n",
    "There is not much to choose here. We have to decide on the parameter schemes for the epislon-greedy policy, the learning rate and the exploring starts. Finally we need to decide how long we want to train for, how many times we want to train and how long we want to evalute for.\n",
    "\n",
    "> *\\_start* indicates the starting value of the parameter.\n",
    ">\n",
    "> *\\_end* indicates the final value of the parameter.\n",
    ">\n",
    "> *\\_cutoff* indicates when the final value is reached, i.e. 0.5 means after 50% of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_learning_params = {\n",
    "        # epsilon-greedy values (linear decay)\n",
    "        \"epsilon_start\": 1,\n",
    "        \"epsilon_end\": 0.05,\n",
    "        \"epsilon_cutoff\": 0.5,\n",
    "\n",
    "        # learning-rate values (exponential decay)\n",
    "        \"alpha_start\": 0.5,\n",
    "        \"alpha_end\": 0.001,\n",
    "        \"alpha_cutoff\": None,\n",
    "\n",
    "        # exploring starts values (linear decay)\n",
    "        \"beta_start\": 1,\n",
    "        \"beta_end\": 0.05,\n",
    "        \"beta_cutoff\": 0.5,\n",
    "        \"exploring_starts\": True\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "        \"n_train\" : 1e5,\n",
    "        \"n_test\" : 1e4,\n",
    "        \"n_runs\" : 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we decide where to save our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming the folder where the results will be saved\n",
    "folder_mode = True\n",
    "folder_name = \"spm_example\"\n",
    "save_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready for the Q-learning!\n",
    "\n",
    "This is easily done with the function *Q\\_learning\\_comparison*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN 1 IN PROGRESS...\n",
      "\tEpisode 20000 (20%), 0:02:15.900000 remaining of this run\n",
      "\tEpisode 40000 (40%), 0:01:31.910000 remaining of this run\n",
      "\tEpisode 60000 (60%), 0:00:55.660000 remaining of this run\n",
      "\tEpisode 80000 (80%), 0:00:26.240000 remaining of this run\n",
      "\tEpisode 100000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER spm_example ALREADY EXISTS\n",
      "...FINISHED IN 0:02:06.430000\n",
      "0:06:19.300000 REMAINING OF THE TRAINING\n",
      "========================================\n",
      "RUN 2 IN PROGRESS...\n",
      "\tEpisode 20000 (20%), 0:02:24.510000 remaining of this run\n",
      "\tEpisode 40000 (40%), 0:01:40.060000 remaining of this run\n",
      "\tEpisode 60000 (60%), 0:01:01.650000 remaining of this run\n",
      "\tEpisode 80000 (80%), 0:00:29.790000 remaining of this run\n",
      "\tEpisode 100000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER spm_example ALREADY EXISTS\n",
      "...FINISHED IN 0:02:26.520000\n",
      "0:04:53.050000 REMAINING OF THE TRAINING\n",
      "========================================\n",
      "RUN 3 IN PROGRESS...\n",
      "\tEpisode 20000 (20%), 0:02:18.700000 remaining of this run\n",
      "\tEpisode 40000 (40%), 0:01:35.560000 remaining of this run\n",
      "\tEpisode 60000 (60%), 0:00:58.220000 remaining of this run\n",
      "\tEpisode 80000 (80%), 0:00:27.570000 remaining of this run\n",
      "\tEpisode 100000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER spm_example ALREADY EXISTS\n",
      "...FINISHED IN 0:02:13.260000\n",
      "0:02:13.260000 REMAINING OF THE TRAINING\n",
      "========================================\n",
      "RUN 4 IN PROGRESS...\n",
      "\tEpisode 20000 (20%), 0:02:15.330000 remaining of this run\n",
      "\tEpisode 40000 (40%), 0:01:32.920000 remaining of this run\n",
      "\tEpisode 60000 (60%), 0:00:56.140000 remaining of this run\n",
      "\tEpisode 80000 (80%), 0:00:26.560000 remaining of this run\n",
      "\tEpisode 100000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER spm_example ALREADY EXISTS\n",
      "...FINISHED IN 0:02:07.830000\n",
      "========================================\n",
      "FULL TRAINING COMPLETED IN 0:08:54.050000\n"
     ]
    }
   ],
   "source": [
    "Q_learning_comparison(\n",
    "    **hyperparams,\n",
    "    args                = model_params,\n",
    "    Q_learning_args     = Q_learning_params,\n",
    "    folder_mode         = folder_mode,\n",
    "    folder_name         = folder_name,\n",
    "    save_mode           = save_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the strategies\n",
    "\n",
    "We can now have a look at the images that were saved when running *Q\\_learning\\_comparison*.\n",
    "\n",
    "Let's first have a look at the reward and the state-value at (0,0) during training.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/simple_model/spm_example/results_graph.png\"/>\n",
    "</div>\n",
    "\n",
    "In this image it looks like that the Q-learning has converged, however, it has not. It has to be trained for longer, which will be evident in the coming images.\n",
    "\n",
    "We can also have a look the learnt strategies. The figure below shows the learnt bid depths.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/simple_model/spm_example/opt_bid_strategy.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Furthermore, we can compare the average rewards of the Q-learning strategies versus benchmarking strategies. These are displayed in the boxplot below.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/simple_model/spm_example/box_plot_benchmarking.png\"/>\n",
    "</div>\n",
    "\n",
    "We can also view these results in table form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy                 mean reward    std reward\n",
      "---------------------  -------------  ------------\n",
      "analytical_discrete        0.127752      0.0764701\n",
      "analytical_continuous      0.132987      0.0717148\n",
      "constant (d=2)             0.0987989     0.0796333\n",
      "random                     0.0618333     0.0958115\n",
      "Q_learning (best run)      0.123278      0.0772611\n",
      "Q_learning (average)       0.127178      0.0785348\n"
     ]
    }
   ],
   "source": [
    "f = open(\"results/simple_model/spm_example/table_benchmarking\")\n",
    "print(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all it looks like the Q-learning has been able to find decent strategies, but it needs to train for longer in order to find strategies that equal the analytical strategies in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More results?\n",
    "\n",
    "There are a lot more figures and tables to explore which can be found in the **[spm_example](https://github.com/KodAgge/Reinforcement-Learning-for-Market-Making/tree/main/code/results/simple_model/spm_example)** folder."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83f73d5875a575e504ba23451a5997fea59c0c75034f677431fe9f5bc2b0207e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
