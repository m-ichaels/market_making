{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Market Making Under a Hawkes Process-Based Limit Order Book Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the core code used in the paper \"*Deep Reinforcement Learning for Market Making Under a Hawkes Process-Based Limit Order Book Model*\" by Bruno Gašperov and Zvonko Kostanjčar, currently under review in the IEEE Control Systems Letters (L-CSS). For any questions feel free to contact us at firstname.lastname@fer.hr (without carons). Hopefully you'll find this notebook useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required packages\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import SAC, TD3\n",
    "import scipy.stats as ss\n",
    "from gym import spaces, logger\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows we define a highly customizable market making environment (simulator):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class marketmakingenv(gym.Env):    \n",
    "    def __init__(self, T=100, dt=1, gamma=0.01, baselineintensities = [0.55,0.55,2.6,2.6,1.5,1.5,1,1], excitationmatrix = [[1.5, 0.4, 3, 0.5, 0, 2, 1, 0],[0.4, 1.5, 0.5, 3, 2, 0, 0, 1],[0.4, 1.2, 0.2, 0, 1.5, 0.5, 0.4, 0.4],[1.2, 0.4, 0, 0.2, 0.5, 1.5, 0.4, 0.4],[0, 0, 2, 0, 0, 0, 0, 0],[0, 0, 0, 2, 0, 0, 0, 0],[1, 0, 2, 0, 0, 1, 3, 0],[0, 1, 0, 2, 1, 0, 0, 3]], inventoryconstraint = 3, delta = 0.01, spread = 0.1, askprice = 100.05,\n",
    "                bidprice = 99.95, s = 100.00, w1 = 8/30, w2=0.25, expdispar = 0.08, mtfee = 0.002, mmfee = 0, decayfactor=10, n1 = 0.45145, n2 = 0.495021633365654, n3 = 0, n4 = 0.5958326386388073, Z = 0.25):\n",
    "        '''\n",
    "        T - terminal time, dt - timestep, gamma - risk aversion parameter, baseline intensities, excitation matrix, inventory constraint,\n",
    "        delta - tick size, spread - initial spread, askprice - initial ask-price, bidprice - initial bidprice, s - initial price\n",
    "        w1 - probability of market maker's market order being aggressive, w2 - probability of market maker's limit order cancellation being aggressive\n",
    "        expdispar - scale of the exponential distribution used for modeling jumps, mtfee - market taker fees, mmfee - market maker fees\n",
    "        decay - beta decay factor (assumed to be fixed), n1 - parameter used for normalization of the spread state space variable\n",
    "        n2 - parameter used for normalization of the spread state space variable, n3 - parameter used for normalization of the trend state space variable\n",
    "        Z - the probability of execution of the market maker’s outstanding limit order standing at the best bid/ask price\n",
    "        '''\n",
    "        super(marketmakingenv, self).__init__()\n",
    "        self.initial_parameters = (spread, askprice, bidprice, s, baselineintensities)\n",
    "        self.t = 0 # initial time\n",
    "        self.s = s # initial midprice value\n",
    "        self.olds = s # initial old midprice value\n",
    "        self.T = T # trading period length (terminal time)\n",
    "        self.dt = dt # time step length\n",
    "        self.q = 0 # initial inventory level\n",
    "        self.oldq = 0 # initial old inventory level\n",
    "        self.x = 0 # initial cash\n",
    "        self.oldx = 0 # initial old cash\n",
    "        self.w1 = w1 # probability of market maker's market order being aggressive\n",
    "        self.w2 = w2 # probability of market maker's limit order cancellation being aggressive\n",
    "        self.n1 = n1 # normalization of the spread state space variable\n",
    "        self.n2 = n2 # normalization of the spread state space variable\n",
    "        self.n3 = n3 # normalization of the trend state space variable\n",
    "        self.n4 = n4 # normalization of the trend state space variable\n",
    "        self.mtfee = mtfee\n",
    "        self.mmfee = mmfee\n",
    "        self.Z = Z\n",
    "        self.expdispar = expdispar # exponential distribution parameter\n",
    "        self.gamma = gamma # risk aversion parameter (part of penalty in the reward function)\n",
    "        self.decayfactor = decayfactor\n",
    "        self.excitationmatrix = excitationmatrix\n",
    "        self.intensities = np.array(baselineintensities)\n",
    "        self.defaultintensities = self.intensities # baseline intensities       \n",
    "        self.done = False # done indicator (indicates whether the episode has finished)\n",
    "        self.inventoryconstraint = inventoryconstraint # inventory constraint\n",
    "        self.delta = delta # tick size\n",
    "        self.spread = spread # initial spread\n",
    "        self.limitscounter = 0 # counts the number of executions of the market maker's limit orders in an episode\n",
    "        self.marketscounter = 0 # counts the number of executions of the market maker's market orders in an episode\n",
    "        self.askprice = askprice # initial ask price\n",
    "        self.bidprice = bidprice # initial bid price\n",
    "        self.ask_order = np.inf # initial ask order\n",
    "        self.bid_order = -np.inf # initial bid order\n",
    "        self.action_space = spaces.Box(np.array([-1, -1]), np.array([1,1]),dtype=np.float32) # action space\n",
    "        self.observation_space = spaces.Box(np.array([-1, -5, -5]), np.array([1, 5, 5]),dtype=np.float32) # observation space\n",
    "        \n",
    "    def step(self, action):\n",
    "        '''Part 1: Canceling the existing (old) market maker's limit orders, if there are any. If such canceling\n",
    "        is aggressive, intensities are updated as well as the bid-, ask-, mid-price and the spread.'''\n",
    "        if (self.ask_order == self.askprice) and random.random() < self.w2: #aggressive limit sell cancelation\n",
    "            jumpsize = np.round(np.random.exponential(self.expdispar)+0.01,2) # shifted exponential distribution\n",
    "            self.askprice = np.round(self.askprice + jumpsize,2) # new ask-price\n",
    "            self.s = np.round((self.askprice+self.bidprice)/2,3) # new mid-price\n",
    "            self.spread = np.round(self.askprice-self.bidprice,2) # new spread\n",
    "            self.intensities += np.array(self.excitationmatrix[5]) # new intensities\n",
    "        if (self.bid_order == self.bidprice) and random.random() < self.w2: #aggressive limit buy cancelation\n",
    "            jumpsize = np.round(np.random.exponential(self.expdispar)+0.01,2) # shifted exponential distribution\n",
    "            self.bidprice = np.round(self.bidprice - jumpsize,2) # new ask-price         \n",
    "            self.s = np.round((self.askprice+self.bidprice)/2,3) # new ask-price\n",
    "            self.spread = np.round(self.askprice-self.bidprice,2) # new spread\n",
    "            self.intensities += np.array(self.excitationmatrix[4]) # new intensities\n",
    "        self.ask_order, self.bid_order = np.inf, -np.inf # cancel all existing orders \n",
    "        self.oldx, self.olds, self.oldq = self.x, self.s, self.q # save old cash, price and inventory\n",
    "        self.reward = 0 # initialize reward\n",
    "        assert self.askprice > self.bidprice # sanity checking\n",
    "        '''Part 2: Setting new limit orders and checking inventory constraints'''\n",
    "        self.ask_order, self.bid_order = np.round(self.askprice + action[0],2), np.round(self.bidprice - action[1],2) # setting new limit orders\n",
    "        if self.q <= -self.inventoryconstraint: # if the side is saturated ignore the ask order\n",
    "            self.ask_order = np.inf\n",
    "        if self.q >= self.inventoryconstraint:  # if the side is saturated ignore the bid order\n",
    "            self.bid_order = -np.inf\n",
    "        if self.ask_order <= self.bid_order: # ignore non-sensical limit orders\n",
    "            self.ask_order, self.bid_order = np.inf, -np.inf #\n",
    "        '''Part 3: Effects of setting the ask limit order.'''\n",
    "        if self.ask_order <= self.bidprice: # treating this case as a market sell\n",
    "            if random.random() < 1-self.w1: # non-aggressive market sell\n",
    "                if self.q > -self.inventoryconstraint: # checking the inventory constraint\n",
    "                    self.q -= 1\n",
    "                    self.marketscounter += 1\n",
    "                    self.x += (1-self.mtfee)*self.bidprice\n",
    "                    self.intensities += np.array(self.excitationmatrix[7])\n",
    "                self.ask_order = np.inf # canceling the ask order\n",
    "                self.bid_order = -np.inf # canceling the bid order\n",
    "            else: # aggressive market sell\n",
    "                if self.q > -self.inventoryconstraint: # checking the inventory constraint\n",
    "                    jumpsize = np.round(np.random.exponential(self.expdispar)+0.01,2)\n",
    "                    self.q -= 1\n",
    "                    self.marketscounter += 1\n",
    "                    self.x += (1-self.mtfee)*(self.bidprice-jumpsize/2) # simplifying assumption\n",
    "                    self.intensities += np.array(self.excitationmatrix[1])\n",
    "                self.ask_order = np.inf # canceling the ask order\n",
    "                self.bid_order = -np.inf # canceling the bid order\n",
    "                self.bidprice = np.round(self.bidprice - jumpsize,2) # new bid-price\n",
    "                self.s = np.round((self.askprice+self.bidprice)/2,3) # new mid-price\n",
    "                self.spread = np.round(self.askprice-self.bidprice,2) # new spread\n",
    "        elif self.ask_order < self.askprice: # agressive ask limit order\n",
    "                self.askprice = np.round(self.ask_order, 2) # new ask-price \n",
    "                self.s = np.round((self.askprice+self.bidprice)/2,3) # new mid-price\n",
    "                self.spread = np.round(self.askprice-self.bidprice,2) # new spread\n",
    "                self.intensities += np.array(self.excitationmatrix[3])\n",
    "        else:\n",
    "                pass\n",
    "        '''Part 4: Effects of setting the bid limit order.'''\n",
    "        if self.bid_order >= self.askprice: # treating this case as a market buy\n",
    "            if random.random() < 1-self.w1: # non-aggressive market buy\n",
    "                if self.q < self.inventoryconstraint: # checking the inventory constraint\n",
    "                    self.q += 1\n",
    "                    self.marketscounter += 1\n",
    "                    self.x -= (1+self.mtfee)*self.askprice\n",
    "                    self.intensities += np.array(self.excitationmatrix[6])\n",
    "                self.ask_order = np.inf # canceling the ask order\n",
    "                self.bid_order = -np.inf # canceling the bid order\n",
    "            else: # aggressive market buy\n",
    "                if self.q < self.inventoryconstraint: # checking the inventory constraint\n",
    "                    jumpsize = np.round(np.random.exponential(self.expdispar)+0.01,2)\n",
    "                    self.q += 1\n",
    "                    self.marketscounter += 1\n",
    "                    self.x -= (1+self.mtfee)*(self.askprice+jumpsize/2) # simplifying assumption \n",
    "                    self.intensities += np.array(self.excitationmatrix[0])\n",
    "                self.ask_order = np.inf # canceling the ask order\n",
    "                self.bid_order = -np.inf # canceling the bid order\n",
    "                self.askprice = np.round(self.askprice + jumpsize,2) # new ask-price\n",
    "                self.s = np.round((self.askprice+self.bidprice)/2,3) # new mid-price\n",
    "                self.spread = np.round(self.askprice-self.bidprice,2) # new spread\n",
    "        elif self.bid_order > self.bidprice: # agressive bid limit order\n",
    "                self.bidprice = np.round(self.bid_order, 2) # new bid-price\n",
    "                self.s = np.round((self.askprice+self.bidprice)/2,3) # new mid-price\n",
    "                self.spread = np.round(self.askprice-self.bidprice,2) # new spread\n",
    "                self.intensities += np.array(self.excitationmatrix[2])\n",
    "        else:\n",
    "                pass \n",
    "        self.reward += self.x+self.q*self.s-self.oldx-self.oldq*self.olds # first part of the reward (directly due to the action, no time has passed)\n",
    "        next_t_change = self.t+self.dt # time of the next time step\n",
    "        while self.t < next_t_change: #as long as the end of the timestep is not reached\n",
    "            time_to_next_event = np.random.exponential(1/sum(self.intensities)) # exponentially distributed time to the next event\n",
    "            event_type = random.choices([1,2,3,4,5,6,7,8], cum_weights=np.cumsum(self.intensities))[0] # the type of the next event\n",
    "            if self.t+time_to_next_event<next_t_change: # if the event time is within the timestep\n",
    "                self.oldx, self.olds, self.oldq = self.x, self.s, self.q # saving the old cash, price and inventory\n",
    "                self.t += time_to_next_event # increase the current time\n",
    "                oldintensities = self.intensities # saving the old intensities\n",
    "                self.intensities = self.defaultintensities*(1-np.exp(-self.decayfactor*time_to_next_event))+self.intensities*np.exp(-self.decayfactor*time_to_next_event) # new intensities\n",
    "                assert self.askprice > self.bidprice # sanity check\n",
    "                if random.random() >= sum(self.intensities)/sum(oldintensities): # discard the event in this case\n",
    "                    event_type = 0\n",
    "                if event_type == 1: #event type 1 => aggressive market buy   \n",
    "                    jumpsize = np.round(np.random.exponential(self.expdispar)+0.01,2) # jump size\n",
    "                    self.askprice = np.round(self.askprice + jumpsize,2) # new ask-price\n",
    "                    self.s = np.round((self.askprice+self.bidprice)/2,3) # new mid-price\n",
    "                    self.spread = np.round(self.askprice-self.bidprice,2) # new spread\n",
    "                    if self.ask_order < self.askprice: # in this case the ask order must have gotten executed\n",
    "                        prob = 1\n",
    "                    elif self.ask_order == self.askprice: # in this case we assume the probability of 25%\n",
    "                        prob = 0.25 \n",
    "                    else: # else it does not get executed\n",
    "                        prob = 0\n",
    "                    if self.ask_order < np.inf and self.q > -self.inventoryconstraint and random.random() < prob: # conditions needed for the execution to take place\n",
    "                        self.q -= 1\n",
    "                        self.limitscounter += 1\n",
    "                        self.x += self.ask_order - self.mmfee*self.ask_order\n",
    "                        self.ask_order = np.inf # ask order is now executed and does not exist anymore\n",
    "                    if self.ask_order < self.askprice:\n",
    "                        self.ask_order = np.inf # the order is canceled (would be executed but can't due to the inventory constraint)\n",
    "                    self.intensities += np.array(self.excitationmatrix[0])\n",
    "                elif event_type == 2: #event type 2 => aggressive market sell    \n",
    "                    jumpsize = np.round(np.random.exponential(self.expdispar)+0.01,2) # jump size\n",
    "                    self.bidprice = np.round(self.bidprice - jumpsize,2) # new bid-price\n",
    "                    self.s = np.round((self.askprice+self.bidprice)/2,3) # new mid-price\n",
    "                    self.spread = np.round(self.askprice-self.bidprice,2) # new spread\n",
    "                    if self.bid_order > self.bidprice: # in this case the bid order must have gotten executed\n",
    "                        prob = 1\n",
    "                    elif self.bid_order == self.bidprice: # in this case we assume the probability of 25%\n",
    "                        prob = 0.25 \n",
    "                    else: # else it does not get executed\n",
    "                        prob = 0\n",
    "                    if self.bid_order > -np.inf and self.q < self.inventoryconstraint and random.random() < prob: # conditions needed for the execution to take place\n",
    "                        self.q += 1\n",
    "                        self.limitscounter += 1\n",
    "                        self.x -= self.bid_order + self.mmfee*self.bid_order #+ 0.006*env.s\n",
    "                        self.bid_order = -np.inf # bid order is now executed and does not exist anymore\n",
    "                    if self.bid_order > self.bidprice:\n",
    "                        self.bid_order = -np.inf # the order is canceled (would be executed but can't due to the inventory constraint)\n",
    "                    self.intensities += np.array(self.excitationmatrix[1])\n",
    "                elif event_type == 3 and (self.askprice-self.bidprice >= np.round(2*self.delta,2)): # event type 3 => aggressive limit buy\n",
    "                    # only happens if the spread is larger than or equals two ticks (otherwise obviously can not happen)\n",
    "                    jumpsize = np.inf\n",
    "                    while jumpsize >= np.round(self.askprice - self.bidprice,2): \n",
    "                        jumpsize = np.round(np.random.exponential(self.expdispar)+0.01,2) # this is in essence a truncated exponential, since there now exists an upper limit to the jump size\n",
    "                    self.bidprice = np.round(self.bidprice + jumpsize,2) # new bid-price\n",
    "                    self.s = np.round((self.askprice+self.bidprice)/2,3) # new mid-price\n",
    "                    self.spread = np.round(self.askprice-self.bidprice,2) # new spread\n",
    "                    self.intensities += np.array(self.excitationmatrix[2]) # new intensities\n",
    "                elif event_type == 4 and (self.askprice-self.bidprice >= np.round(2*self.delta,2)): # event type 4 => aggressive limit sell\n",
    "                    jumpsize = np.inf\n",
    "                    while jumpsize >= np.round(self.askprice - self.bidprice,2):\n",
    "                        jumpsize = np.round(np.random.exponential(self.expdispar)+0.01,2) # this is in essence a truncated exponential, since there now exists an upper limit to the jump size\n",
    "                    self.askprice = np.round(self.askprice - jumpsize,2) # new ask-price\n",
    "                    self.s = np.round((self.askprice+self.bidprice)/2,3) # new mid-price\n",
    "                    self.spread = np.round(self.askprice-self.bidprice,2) # new spread\n",
    "                    self.intensities += np.array(self.excitationmatrix[3]) # new intensities\n",
    "                elif event_type == 5 and self.bidprice>=np.round(self.bid_order+self.delta,2): # event type 5 => agressive limit buy cancellation\n",
    "                    jumpsize = np.inf\n",
    "                    if self.bid_order>-np.inf: # if the market maker's bid order still exists, it provides an upper bound to the jump size\n",
    "                        while jumpsize > np.round(self.bidprice-self.bid_order,2):\n",
    "                            jumpsize = np.round(np.random.exponential(self.expdispar)+0.01,2) # generating jump size\n",
    "                    else: # in this case there is no upper bound to the jump size\n",
    "                        jumpsize = np.round(np.random.exponential(self.expdispar)+0.01,2) # generating jump size\n",
    "                    self.bidprice = np.round(self.bidprice - jumpsize,2) # new bid-price\n",
    "                    self.s = np.round((self.askprice+self.bidprice)/2,3) # new mid-price\n",
    "                    self.spread = np.round(self.askprice-self.bidprice,2) # new spread\n",
    "                    self.intensities += np.array(self.excitationmatrix[4]) # new intensities\n",
    "                elif event_type == 6 and self.askprice<=np.round(self.ask_order-self.delta,2):  # event type 6 => agressive limit sell cancellation\n",
    "                    jumpsize = np.inf\n",
    "                    if self.ask_order<np.inf: # if the market maker's ask order still exists, it provides an upper bound to the jump size\n",
    "                        while jumpsize > np.round(self.ask_order-self.askprice,2):\n",
    "                            jumpsize = np.round(np.random.exponential(self.expdispar)+0.01,2) # generating jump size\n",
    "                    else: # in this case there is no upper bound to the jump size\n",
    "                        jumpsize = np.round(np.random.exponential(self.expdispar)+0.01,2) # generating jump size\n",
    "                    self.askprice = np.round(self.askprice + jumpsize,2) # new ask-price\n",
    "                    self.s = np.round((self.askprice+self.bidprice)/2,3) # new mid-price\n",
    "                    self.spread = np.round(self.askprice-self.bidprice,2) # new spread\n",
    "                    self.intensities += np.array(self.excitationmatrix[5])  # new intensities\n",
    "                elif event_type == 7: # event type 7 => non-aggressive market buy\n",
    "                    if self.ask_order == self.askprice and self.q > -self.inventoryconstraint and random.random()<self.Z: # checking conditions\n",
    "                        self.q -= 1\n",
    "                        self.limitscounter += 1\n",
    "                        self.x += self.ask_order - self.mmfee*self.ask_order\n",
    "                        self.ask_order = np.inf\n",
    "                    self.intensities += np.array(self.excitationmatrix[6]) # new intensities\n",
    "                elif event_type == 8: # event type 8 => non-aggressive market sell\n",
    "                    if self.bid_order == self.bidprice and self.q < self.inventoryconstraint and random.random()<self.Z: # checking conditions\n",
    "                        self.q += 1\n",
    "                        self.limitscounter += 1\n",
    "                        self.x -= self.bid_order + self.mmfee*self.bid_order\n",
    "                        self.bid_order = -np.inf\n",
    "                    self.intensities += np.array(self.excitationmatrix[7]) # new intensities\n",
    "                else:\n",
    "                    pass\n",
    "                self.reward += self.x+self.q*self.s-self.oldx-self.oldq*self.olds-self.gamma*time_to_next_event*abs(self.oldq) # the reward function as explained in the paper\n",
    "            else: # if the event time exceeds the end of the time step\n",
    "                time_to_next_event=next_t_change-self.t\n",
    "                self.t += time_to_next_event # setting time to beginning of the next time-step\n",
    "                self.intensities = self.defaultintensities*(1-np.exp(-self.decayfactor*time_to_next_event))+self.intensities*np.exp(-self.decayfactor*time_to_next_event) #updating intensities\n",
    "                self.reward -= self.gamma*time_to_next_event*abs(self.q) # reward function\n",
    "        self.state = np.array([self.q/self.inventoryconstraint, (self.spread-self.n1)/self.n2, (self.intensities[0]+self.intensities[-2]-self.intensities[1]-self.intensities[-1]-self.n3)/self.n4]) # state space formulation\n",
    "        if self.t >= self.T: # if the end of the episode is reached\n",
    "            self.done = True\n",
    "        return self.state, self.reward, self.done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0 # initial time\n",
    "        self.s = self.initial_parameters[3] # initial midprice value\n",
    "        self.olds = self.initial_parameters[3] # initial old midprice value\n",
    "        self.q = 0 # initial inventory level\n",
    "        self.oldq = 0 # initial old inventory level\n",
    "        self.x = 0 # initial cash\n",
    "        self.oldx = 0 # initial old cash\n",
    "        self.intensities = np.array(self.initial_parameters[4])\n",
    "        self.defaultintensities = self.intensities # baseline intensities\n",
    "        self.done = False # done indicator (indicates whether the episode has finished)\n",
    "        self.limitscounter = 0  # counts the number of executions of the market maker's limit orders in an episode\n",
    "        self.marketscounter = 0 # counts the number of executions of the market maker's market orders in an episode\n",
    "        self.spread = self.initial_parameters[0] # initial spread\n",
    "        self.askprice = self.initial_parameters[1] # initial ask price\n",
    "        self.bidprice = self.initial_parameters[2] # initial bid price\n",
    "        self.ask_order = np.inf # initial ask order\n",
    "        self.bid_order = -np.inf # initial bid order\n",
    "        self.state = np.array([self.q/self.inventoryconstraint, (self.spread-self.n1)/self.n2, (self.intensities[0]+self.intensities[-2]-self.intensities[1]-self.intensities[-1]-self.n3)/self.n4]) # state space (normalized)\n",
    "        return self.state \n",
    " \n",
    "    def render(self):\n",
    "        print(self.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining normalization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_normalization_parameters(T=10000, n1=0, n2=1, n3=0, n4=1):\n",
    "    spreads = []\n",
    "    alphas = []\n",
    "    \n",
    "    env = marketmakingenv(T=T, n1=n1, n2=n2, n3=n3, n4=n4)\n",
    "    for i in range(1):\n",
    "        observation = env.reset()\n",
    "        while True:\n",
    "            spreads.append(env.state[1])\n",
    "            alphas.append(env.state[2])\n",
    "            action = [np.random.uniform(0,1),np.random.uniform(0,1)]\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "    return np.mean(spreads), np.std(spreads), np.mean(alphas), np.std(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maver\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "n1_, n2_, n3_, n4_ = return_normalization_parameters(T=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = marketmakingenv(n1=n1_, n2=n2_, n3=n3_, n4=n4_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the SAC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maver\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = SAC(\"MlpPolicy\",\n",
    "            env,\n",
    "            verbose=1,\n",
    "            gamma=1,\n",
    "            batch_size=512,\n",
    "            learning_rate=0.0003,\n",
    "            policy_kwargs=dict(\n",
    "                activation_fn=nn.ReLU,\n",
    "                net_arch=[64,64]))#, buffer_size=100000, batch_size=512)# policy_kwargs=dict(act_fun=tf.nn.tanh, layers=[12, 12]))#, nstep=64*8,  buffer_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maver\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.set_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 76.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 41       |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 400      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -29.3    |\n",
      "|    critic_loss     | 14.3     |\n",
      "|    ent_coef        | 0.0581   |\n",
      "|    ent_coef_loss   | -0.0837  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14153    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 90.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 800      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -30.1    |\n",
      "|    critic_loss     | 8.15     |\n",
      "|    ent_coef        | 0.0595   |\n",
      "|    ent_coef_loss   | 0.199    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14553    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 87.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 1200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -31.2    |\n",
      "|    critic_loss     | 8.89     |\n",
      "|    ent_coef        | 0.0605   |\n",
      "|    ent_coef_loss   | 0.172    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14953    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 94.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 1600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -32      |\n",
      "|    critic_loss     | 8.9      |\n",
      "|    ent_coef        | 0.0603   |\n",
      "|    ent_coef_loss   | -0.156   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 15353    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 93.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -32.9    |\n",
      "|    critic_loss     | 7.47     |\n",
      "|    ent_coef        | 0.0607   |\n",
      "|    ent_coef_loss   | 0.466    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 15753    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 90.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 2400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -33.7    |\n",
      "|    critic_loss     | 15.5     |\n",
      "|    ent_coef        | 0.0627   |\n",
      "|    ent_coef_loss   | 0.119    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 16153    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 91.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 2800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -34.5    |\n",
      "|    critic_loss     | 17.5     |\n",
      "|    ent_coef        | 0.0633   |\n",
      "|    ent_coef_loss   | -0.158   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 16553    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 89.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 91       |\n",
      "|    total_timesteps | 3200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -35.5    |\n",
      "|    critic_loss     | 17.6     |\n",
      "|    ent_coef        | 0.0615   |\n",
      "|    ent_coef_loss   | -0.161   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 16953    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 89       |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 104      |\n",
      "|    total_timesteps | 3600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -36.2    |\n",
      "|    critic_loss     | 15.7     |\n",
      "|    ent_coef        | 0.0626   |\n",
      "|    ent_coef_loss   | -0.0824  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 17353    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 89.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 114      |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -37.2    |\n",
      "|    critic_loss     | 11.8     |\n",
      "|    ent_coef        | 0.0621   |\n",
      "|    ent_coef_loss   | -0.452   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 17753    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 88.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 127      |\n",
      "|    total_timesteps | 4400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -38      |\n",
      "|    critic_loss     | 5.7      |\n",
      "|    ent_coef        | 0.0609   |\n",
      "|    ent_coef_loss   | -0.0579  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18153    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 87.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 138      |\n",
      "|    total_timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -38.5    |\n",
      "|    critic_loss     | 12.9     |\n",
      "|    ent_coef        | 0.0609   |\n",
      "|    ent_coef_loss   | -0.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18553    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 88.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 152      |\n",
      "|    total_timesteps | 5200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -39.4    |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    ent_coef        | 0.0602   |\n",
      "|    ent_coef_loss   | -0.0444  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18953    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 90.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 164      |\n",
      "|    total_timesteps | 5600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -40.1    |\n",
      "|    critic_loss     | 10.5     |\n",
      "|    ent_coef        | 0.0598   |\n",
      "|    ent_coef_loss   | 0.393    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19353    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 91.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 178      |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -41.1    |\n",
      "|    critic_loss     | 17       |\n",
      "|    ent_coef        | 0.0619   |\n",
      "|    ent_coef_loss   | -0.0381  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19753    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 91.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 190      |\n",
      "|    total_timesteps | 6400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -41.8    |\n",
      "|    critic_loss     | 7.82     |\n",
      "|    ent_coef        | 0.0635   |\n",
      "|    ent_coef_loss   | -0.256   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 20153    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 91.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 204      |\n",
      "|    total_timesteps | 6800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -42.3    |\n",
      "|    critic_loss     | 16.6     |\n",
      "|    ent_coef        | 0.0626   |\n",
      "|    ent_coef_loss   | 0.099    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 20553    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 90.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 217      |\n",
      "|    total_timesteps | 7200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -42.9    |\n",
      "|    critic_loss     | 24.9     |\n",
      "|    ent_coef        | 0.0642   |\n",
      "|    ent_coef_loss   | -0.0143  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 20953    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 91.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 231      |\n",
      "|    total_timesteps | 7600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -44      |\n",
      "|    critic_loss     | 29.9     |\n",
      "|    ent_coef        | 0.0646   |\n",
      "|    ent_coef_loss   | -0.0473  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 21353    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 91.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 244      |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -44.5    |\n",
      "|    critic_loss     | 12.8     |\n",
      "|    ent_coef        | 0.0654   |\n",
      "|    ent_coef_loss   | -0.0868  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 21753    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 91.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 255      |\n",
      "|    total_timesteps | 8400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -45      |\n",
      "|    critic_loss     | 13.2     |\n",
      "|    ent_coef        | 0.064    |\n",
      "|    ent_coef_loss   | -0.386   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 22153    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 91.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 268      |\n",
      "|    total_timesteps | 8800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -45.7    |\n",
      "|    critic_loss     | 18.4     |\n",
      "|    ent_coef        | 0.0659   |\n",
      "|    ent_coef_loss   | -0.398   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 22553    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 91       |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 278      |\n",
      "|    total_timesteps | 9200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -46.2    |\n",
      "|    critic_loss     | 37.4     |\n",
      "|    ent_coef        | 0.0659   |\n",
      "|    ent_coef_loss   | 0.182    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 22953    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 90.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 288      |\n",
      "|    total_timesteps | 9600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -47.1    |\n",
      "|    critic_loss     | 18.7     |\n",
      "|    ent_coef        | 0.0659   |\n",
      "|    ent_coef_loss   | 0.136    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 23353    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 90.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 301      |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -47.8    |\n",
      "|    critic_loss     | 15       |\n",
      "|    ent_coef        | 0.0642   |\n",
      "|    ent_coef_loss   | -0.127   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 23753    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x13a014b6b40>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(10000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and loading models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model1\")\n",
    "model = SAC.load(\"model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained agent\n",
    "def tester():\n",
    "    rreturns3 = []\n",
    "    pvalues3 = []\n",
    "    qqs3 = []\n",
    "    qqsw = []\n",
    "    ppvs = []\n",
    "    allqs = []\n",
    "    ssss = []\n",
    "    #added_spreads = []\n",
    "    for i in range(1000):\n",
    "        obs = env.reset()\n",
    "        returns3 = []\n",
    "        qs3 = []\n",
    "        pvs= []\n",
    "        #added_spread = []\n",
    "        while True:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            returns3.append(reward)\n",
    "            pvs.append(env.x+env.q*env.s)\n",
    "            qs3.append(env.q)\n",
    "            if done:\n",
    "                pvalues3.append(env.x+env.q*env.s)\n",
    "                break\n",
    "                #print(\"Episode end\")\n",
    "        qqs3.append(qs3[-1])\n",
    "        allqs.append(qs3)\n",
    "        ppvs.append(pvs)\n",
    "        qqsw.append(np.mean(abs(np.array(qs3))))\n",
    "        rreturns3.append(np.sum(returns3))\n",
    "        ssss.append(env.s)\n",
    "        #added_spreads.append(np.mean(added_spread))\n",
    "    return rreturns3, pvalues3, qqs3, qqsw, ppvs, allqs, ssss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rreturns1, pvalues1, qqs1, qqsw1, ppvs1, allqs1, ssss1 = tester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return:  96.07001015879982\n",
      "Average final value:  98.08315848\n"
     ]
    }
   ],
   "source": [
    "print(\"Average return: \", np.mean(rreturns1))\n",
    "print(\"Average final value: \", np.mean(pvalues1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
